import re
from utils.chat.logger import log
import json

    
def create_specified_pages_dict(final_response):

    matches = re.findall(r'\[Document:\s*(.*?)\s*\|\s*(PAGE_\d+(?:,\s*PAGE_\d+)*)\]', final_response)
    

    result = {}
    

    for document, pages in matches:
     
        page_numbers = re.findall(r'PAGE_(\d+)', pages)
        page_numbers = [int(page) for page in page_numbers]
        
        if document in result:
            result[document].extend(page_numbers)
        else:
            result[document] = page_numbers
    

    for document in result:
        result[document] = sorted(set(result[document]))
    
    return result

def clean_sources(context, specified_pages_dict):
    def extract_text_chunks(context):
        pattern = re.compile(r"\*\*Document:\s*(?P<document_name>.*?)\s*\|\s*PAGE_(?P<page_number>\d+)\*\*\n(?P<text_chunk>.*?)(?=\*\*Document|\Z)", re.DOTALL)
        matches = pattern.finditer(context)

        chunks_list = []
        for match in matches:
            document_name = match.group('document_name').strip()
            page_number = int(match.group('page_number'))
            text_chunk = match.group('text_chunk').strip()

            chunks_list.append({
                "document_name": f'{document_name}',
                "page_number": page_number,
                "text_chunk": text_chunk
            })
        return chunks_list

    chunks_list = extract_text_chunks(context)
    result = []
    seen_chunks = set()

   
    page_dict = {}
    for chunk in chunks_list:
        document_name = chunk['document_name']
        page_number = chunk['page_number']
        if document_name not in page_dict:
            page_dict[document_name] = {}
        if page_number not in page_dict[document_name]:
            page_dict[document_name][page_number] = []
        page_dict[document_name][page_number].append(chunk)

   
    for document_name, pages in specified_pages_dict.items():
        if document_name in page_dict:
            for page in pages:
                if page in page_dict[document_name]:
                    for chunk in page_dict[document_name][page]:
                        chunk_id = (chunk['document_name'], chunk['page_number'], chunk['text_chunk'])
                        if chunk_id not in seen_chunks:
                            result.append(chunk)
                            seen_chunks.add(chunk_id)
                    del page_dict[document_name][page]
    
    result_string = ''
    result_dictionery = {}
    for chunk in result:
        current_document_name = chunk['document_name']
        current_page_number = chunk['page_number']
        current_chunk_text = chunk['text_chunk']

        result_string += f"**Document: {current_document_name} | PAGE_{current_page_number}**\n\n{current_chunk_text}\n\n"
        '''
        now make the result dictionery look like:
        result_dictionery = {
            'document_name': {
                'page_number': 'text_chunk',
                'page_number': 'text_chunk',
                and so on for every page number ...
                },git

            and so on for every document...
        }
        '''
      
        if current_document_name not in result_dictionery:
            result_dictionery[current_document_name] = {}

        result_dictionery[current_document_name][current_page_number] = current_chunk_text

    return result_string, result_dictionery

def remove_html_tags(text):
    log("Removing HTML tags")
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

async def find_source_document_and_pages(text_chunk):
    """
    Extract document references from text chunks generated by LLM.

    Args:
        text_chunk (str): Text containing document references in JSON format

    Returns:
        tuple: (document_name, document_pages_list)
    """
    try:
        doc_info = json.loads(text_chunk)
        for doc_name, pages in doc_info.items():
          if not isinstance(pages, list):
                raise ValueError(f"Pages for document '{doc_name}' must be a list.")
        return doc_info
    except (json.JSONDecodeError, KeyError) as e:
        print("Error parsing document info:", e)
        return None, None

